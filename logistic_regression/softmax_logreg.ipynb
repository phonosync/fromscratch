{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Logistic Regression\n",
    "Multinomial logistic regression for more than 2 classes, but still, each sample can only belong to one class.\n",
    "\n",
    "## The Model\n",
    "\n",
    "\n",
    "\n",
    "## Implementation of the Feed-Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(zet):\n",
    "    zet -= np.max(zet)\n",
    "    sm = (np.exp(zet).T / np.sum(np.exp(zet),axis=1)).T\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z = Xb.dot(W)\n",
    "Y = softmax(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "Multinomial logistic regression has a slightly different loss function than binary logistic regression because it uses the softmax rather than the sigmoid classifier. Consider for all training samples $\\{x_n\\}$ and classes k:\n",
    "$t_{nk} = 1$ if the target for sample $n$ is of class $k$ and $=0$ otherwise. \n",
    "\n",
    "$$L_{\\mathbf{W}, b} \\stackrel{\\text{def}}{=} \\prod_{n=1}^N \\prod_{k=1}^K y_{nk}^{\\ \\Large t_{nk}}$$\n",
    "\n",
    "The Loss to be minimised is the negative log-likelihood, also called Categorical Cross Entropy Loss:\n",
    "\n",
    "$$L_{CE}\\stackrel{\\text{def}}{=} - \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk} $$\n",
    "\n",
    "If the more $y_{nk}$ is wrong, the larger the loss: Consider just 1 sample:\n",
    "\n",
    "* Exactly right $\\; \\rightarrow \\; L_{CE}=0$\n",
    "* 50\\% probability on correct target $\\; \\rightarrow \\; L_{CE}=-1*\\cdot\\log(0.5)=0.693$\n",
    "* 25\\% probability on correct target $\\; \\rightarrow \\; L_{CE}=-1*\\cdot\\log(0.25)=1.386$\n",
    "* 0\\% probability on correct target $\\; \\rightarrow \\; L_{CE}=-1*\\cdot\\log(0)=\\infty$\n",
    "\n",
    "\n",
    "Or maximise the the log-likelihood:\n",
    "\n",
    "$$J = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk}$$\n",
    "\n",
    "## Optimisation\n",
    "There is no closed-form solution for the optimal $W$ in softmax logistic regression $\\rightarrow$ find the partial derivaties of $J$ with respect to the $W_{dk}$ and perform gradient descent.\n",
    "\n",
    "$$z=W^Tx$$\n",
    "$$y=softmax(z)$$\n",
    "$$y_{nk} = \\frac{e^{z_{nk}}}{\\sum_{k'=1}^K e^{z_{nk'}}}$$\n",
    "\n",
    "$$J = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk}$$\n",
    "\n",
    "partial derivative with respect to weight that connects $x_d$ with \"class\" $k$:\n",
    "$$\\frac{\\partial J}{\\partial W_{dk}} = \\sum_{n=1}^N \\sum_{i=1}^K \\frac{\\partial J_{ni}}{\\partial y_{ni}} \\frac{\\partial y_{ni}}{\\partial z_{nk}} \\frac{\\partial z_{nk}}{\\partial W_{dk}}$$\n",
    "\n",
    "$$J_{ni} = t_{ni} \\log y_{ni}$$\n",
    "$$\\frac{\\partial J_{ni}}{\\partial y_{ni}} = \\frac{t_ni}{y_{ni}}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_{ni} &=\\frac{e^{z_{ni}}}{\\sum_j e^{z_{nj}}}\\\\\n",
    "&= e^{z_{ni}}\\left(\\sum_j e^{z_{nj}}\\right)^{-1}\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial y_{ni}}{\\partial z_{nk}} &= (-1)  e^{z_{ni}} \\left(\\sum_j e^{z_{nj}}\\right)^{-2}  e^{z_{nk}} \\; \\; \\; \\; \\text{if} \\; i\\neq k\\\\\n",
    "&= (-1) \\frac{e^{z_{ni}}}{\\sum_j e^{z_{nj}}} \\frac{e^{z_{nk}}}{\\sum_j e^{z_{nj}}}\\\\\n",
    "&= -y_{ni}y_{nk}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial y_{ni}}{\\partial z_{nk}} &=  e^{z_{nk}} \\left(\\sum_j e^{z_{nj}}\\right)^{-1} -  e^{z_{nk}} \\left(\\sum_j e^{z_{nj}}\\right)^{-2} e^{z_{nk}} \\; \\; \\; \\; \\text{if} \\; i= k\\\\\n",
    "&= y_{ni}(1-y_{nk}) \\; \\; \\; \\; \\text{although} \\; k=i\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "with\n",
    "$$\\begin{aligned}\n",
    "\\delta_{ki} &= 1 \\; \\; \\; \\; \\text{if} \\; i= k \\\\\n",
    "\\delta_{ki} &= 0 \\; \\; \\; \\; \\text{if} \\; i\\neq k\n",
    "\\end{aligned}$$\n",
    "the expression simplifies to\n",
    "$$\\frac{\\partial y_{ni}}{\\partial z_{nk}} = y_{ni}(\\delta_{ki}-y_{nk})$$\n",
    "\n",
    "\n",
    "$$z_{nk} = W_{:,k}^{T}x_n$$\n",
    "$$\\frac{\\partial z_{nk}}{\\partial W_{dk}} = x_{nd}$$\n",
    "\n",
    "combine and simplify\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial W_{dk}} &= \\sum_{n=1}^N \\sum_{i=1}^K \\frac{t_{ni}}{y_{ni}} y_{ni}(\\delta_{ki}-y_{nk}) x_{nd}\\\\\n",
    "&= \\sum_{n=1}^N \\sum_{i=1}^K t_{ni}(\\delta_{ki}-y_{nk}) x_{nd}\\\\\n",
    "&= \\sum_{n=1}^N x_{nd}\\left( \\sum_{i=1}^K t_{ni}\\delta_{ki}- \\sum_{i=1}^K t_{ni} y_{nk} \\right)\\\\\n",
    "&= \\sum_{n=1}^N \\left(t_{nk} - y_{nk}\\right) x_{nd}\n",
    "\\end{aligned}$$\n",
    "\n",
    "In matrix form:\n",
    "$$\\nabla J=X^T(T-Y)$$\n",
    "\n",
    "In code:\n",
    "```python\n",
    "X.T.dot(T-Y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
