{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with Naive Bayes\n",
    "Based on [Jurafsky: Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "\n",
    "Multinomial Naive Bayes: The class priors are assumed to follow a multinomial distribution - common for text classification.\n",
    "\n",
    "Assumes Bag-of-Words: Documents are represented by the unordered collection of their word counts. Text sequence information is ignored.\n",
    "## The Model\n",
    "* Corpus of $N$ documents $\\{d_n\\}_1^N$\n",
    "* Vocabulary $V$: set of words $\\{w_j\\}_1^J$\n",
    "* Feature based on count of vocabulary word $w_j$ in document $d_n$: $f_{nj}=f(w_j;d_n)$  \n",
    "* $K$ Classes $\\{c_k\\}_1^K$\n",
    "\n",
    "The Naive Bayes classifier returns the class $\\hat{c}_n$ which has the maximum posterior probability given the document $d_n$.\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\hat{c}_n=\\argmax_{\\{c_k\\}} P(c_k|d_n)\n",
    "$$\n",
    "\n",
    "Bayes' Theorem says that the conditional probability $P(c_k|d_n)$ to observe class $c_k$ given document $d_n$ can be decomposed into the product of the likelihood $P(d_n|c_k)$ and the prior $P(c_k)$ (the probability for class $c_k$ without any observation)\n",
    "$$\\hat{c}_n=\\argmax_{\\{c_k\\}} P(c_k|d_n)=\\argmax_{\\{c_k\\}} \\frac{P(d_n|c_k)P(c_k)}{P(d_n)}$$\n",
    "\n",
    "$P(d_n)$ can be dropped, because it is the same for all classes $\\{c_k\\}_1^K$.\n",
    "\n",
    "$$\\hat{c}_n=\\argmax_{\\{c_k\\}} P(c_k|d_n)=\\argmax_{\\{c_k\\}} \\overbrace{P(d_n|c_k)}^{\\text{likelihood}}\\overbrace{P(c_k)}^{\\text{prior}}$$\n",
    "\n",
    "Represent document $d_n$ as a set of features $f_{n1}, f_{n2},...,f_{nJ}$, which encode the presence of the respective vocabulary words $w_1, w_2,...,w_J$ in document $d_n$  \n",
    "\n",
    "$$\\hat{c}=\\argmax_{\\{c_k\\}} P(f_{n1}, f_{n2},...,f_{nJ}|c_k)P(c_k)$$\n",
    "\n",
    "Naive assumption: the probabilities $P(f_{nj}|c_k)$ are independent given the class $c_k$ and therefore\n",
    "\n",
    "$$P(f_{n1}, f_{n2},...,f_{nJ}|c_k) = P(f_{n1}|c_k)\\cdot P(f_{n2}|c_k)\\cdot ... \\cdot P(f_{nJ}|c_k)$$\n",
    "\n",
    "$$c_n^{NB}=\\argmax_{\\{c_k\\}} P(c_k) \\prod_{j} P(f_{nj}|c_k)$$\n",
    "\n",
    "in log space to avoid underflow and increase speed:\n",
    "\n",
    "$$c_n^{NB}=\\argmax_{\\{c_k\\}} \\log P(c_k) + \\sum_{j} \\log P(f_{nj}|c_k)$$\n",
    "\n",
    "## Application to Word Tokens of Document $d_n$\n",
    "Applying this to word tokens of document $d_n$ $\\{t_i\\}$ requires to look up the corresponding word in the vocabulary $w_{j}$ for each token $t_i$. Tokens without corresponding word in the vocabulary are simply ignored.\n",
    "\n",
    "$$c_n^{NB}=\\argmax_{\\{c_k\\}} \\log P(c_k) + \\sum_{t_i} \\log P(w_j|c_k)$$\n",
    "\n",
    "## Training\n",
    "\n",
    "$$\\hat{P}(c_k)=\\frac{N_k}{N}$$\n",
    "$N_k$ the number of documents of class $c_k$ divided by the total number of documents (samples).\n",
    "\n",
    "$P(w_{j}|c_k)$ is computed as the fraction of times the vocabulary word $w_j$ appears among all tokens in all documents of class $c_k$\n",
    "\n",
    "$$P(w_{j}|c_k)=\\frac{count(w_j,c_k)}{\\sum_{w_{j'}\\in V}count(w_{j'},c_k)}$$\n",
    "\n",
    "Apply Smoothing $\\alpha$ to avoid zero probabilites for vocabulary words that do not appear in all classes.\n",
    "\n",
    "$$P(w_j|c_k)=\\frac{count(w_j,c_k)+\\alpha}{\\sum_{w\\in V}(count(w,c_k)+\\alpha)}=\\frac{count(w_j,c_k)+\\alpha}{\\left(\\sum_{w\\in V}(count(w,c_k)\\right)+\\alpha\\cdot|V|}$$\n",
    "\n",
    "Laplace Smoothing: $\\alpha = 1$  \n",
    "Lidstone Smoothing: $0 < \\alpha < 1$ \n",
    "\n",
    "Words missing in the vocabulary are simply ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Sentiment Classification task (0: Negative, 1: Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['just plain boring',\n",
    "         'entirely predictable and lacks energy',\n",
    "         'no surprises and very few laughs',\n",
    "         'very powerful',\n",
    "         'the most fun file of the summer'\n",
    "        ]\n",
    "\n",
    "classes = ['-',\n",
    "     '-',\n",
    "     '-',\n",
    "     '+',\n",
    "     '+'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs)\n",
    "clss = list(set(classes))\n",
    "K = len(clss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Illustrative Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organise the Target Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_cls = {label:i for i,label in enumerate(clss)}\n",
    "\n",
    "T = np.array([map_cls[label] for label in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', '-']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Log Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 3.0]\n",
      "[-0.916290731874155, -0.5108256237659907]\n"
     ]
    }
   ],
   "source": [
    "log_priors = [0.0]*K\n",
    "for t in T:\n",
    "    log_priors[t] += 1\n",
    "print(log_priors)\n",
    "log_priors = [np.log(count/N) for count in log_priors]\n",
    "print(log_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'surprises': 1,\n",
       " 'just': 2,\n",
       " 'boring': 3,\n",
       " 'file': 4,\n",
       " 'few': 5,\n",
       " 'no': 6,\n",
       " 'lacks': 7,\n",
       " 'summer': 8,\n",
       " 'of': 9,\n",
       " 'fun': 10,\n",
       " 'very': 11,\n",
       " 'plain': 12,\n",
       " 'entirely': 13,\n",
       " 'energy': 14,\n",
       " 'predictable': 15,\n",
       " 'the': 16,\n",
       " 'laughs': 17,\n",
       " 'most': 18,\n",
       " 'powerful': 19}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = set([token for doc in docs for token in doc.split()])\n",
    "n_V = len(V)\n",
    "\n",
    "map_V = {word:pos for pos, word in enumerate(V)}\n",
    "map_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training documents into word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_word_counts(doc):\n",
    "    counts = [0]*n_V\n",
    "    for token in doc.split():\n",
    "        try:\n",
    "            counts[map_V[token]] += 1\n",
    "        except KeyError: # simply ignore words not in the vocabulary\n",
    "            pass\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([doc_to_word_counts(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ now contains for each document (row) the counts per vocabulary word (column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>surprises</th>\n",
       "      <th>just</th>\n",
       "      <th>boring</th>\n",
       "      <th>file</th>\n",
       "      <th>few</th>\n",
       "      <th>no</th>\n",
       "      <th>lacks</th>\n",
       "      <th>summer</th>\n",
       "      <th>of</th>\n",
       "      <th>fun</th>\n",
       "      <th>very</th>\n",
       "      <th>plain</th>\n",
       "      <th>entirely</th>\n",
       "      <th>energy</th>\n",
       "      <th>predictable</th>\n",
       "      <th>the</th>\n",
       "      <th>laughs</th>\n",
       "      <th>most</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just plain boring</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entirely predictable and lacks energy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no surprises and very few laughs</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very powerful</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the most fun file of the summer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  surprises  just  boring  file  \\\n",
       "just plain boring                        0          0     1       1     0   \n",
       "entirely predictable and lacks energy    1          0     0       0     0   \n",
       "no surprises and very few laughs         1          1     0       0     0   \n",
       "very powerful                            0          0     0       0     0   \n",
       "the most fun file of the summer          0          0     0       0     1   \n",
       "\n",
       "                                       few  no  lacks  summer  of  fun  very  \\\n",
       "just plain boring                        0   0      0       0   0    0     0   \n",
       "entirely predictable and lacks energy    0   0      1       0   0    0     0   \n",
       "no surprises and very few laughs         1   1      0       0   0    0     1   \n",
       "very powerful                            0   0      0       0   0    0     1   \n",
       "the most fun file of the summer          0   0      0       1   1    1     0   \n",
       "\n",
       "                                       plain  entirely  energy  predictable  \\\n",
       "just plain boring                          1         0       0            0   \n",
       "entirely predictable and lacks energy      0         1       1            1   \n",
       "no surprises and very few laughs           0         0       0            0   \n",
       "very powerful                              0         0       0            0   \n",
       "the most fun file of the summer            0         0       0            0   \n",
       "\n",
       "                                       the  laughs  most  powerful  \n",
       "just plain boring                        0       0     0         0  \n",
       "entirely predictable and lacks energy    0       0     0         0  \n",
       "no surprises and very few laughs         0       1     0         0  \n",
       "very powerful                            0       0     0         1  \n",
       "the most fun file of the summer          2       0     1         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = pd.DataFrame(X, columns=V)\n",
    "df_X.index = docs\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Calculate the Likelihoods for the vocabulary words given class $c$\n",
    "$$P(w_{i}|c)=\\frac{count(w_i,c)+\\alpha}{\\sum_{w\\in V}(count(w,c)+\\alpha)}$$\n",
    "\n",
    "First calculate the  $count(w_i,c)+\\alpha$ for each class $c$ and vocabulary word $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_w_c = np.ones((K,n_V)) * alpha  # initialise with the +alpha counts for the Smoothing\n",
    "for label,i in map_cls.items():  # loop over all labels and their positions in the cls array\n",
    "    for doc in X[T == i]:  # select all docs of class c\n",
    "        counts_w_c[i] += doc  # and add up the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>surprises</th>\n",
       "      <th>just</th>\n",
       "      <th>boring</th>\n",
       "      <th>file</th>\n",
       "      <th>few</th>\n",
       "      <th>no</th>\n",
       "      <th>lacks</th>\n",
       "      <th>summer</th>\n",
       "      <th>of</th>\n",
       "      <th>fun</th>\n",
       "      <th>very</th>\n",
       "      <th>plain</th>\n",
       "      <th>entirely</th>\n",
       "      <th>energy</th>\n",
       "      <th>predictable</th>\n",
       "      <th>the</th>\n",
       "      <th>laughs</th>\n",
       "      <th>most</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  surprises  just  boring  file  few   no  lacks  summer   of  fun  \\\n",
       "+  1.0        1.0   1.0     1.0   2.0  1.0  1.0    1.0     2.0  2.0  2.0   \n",
       "-  3.0        2.0   2.0     2.0   1.0  2.0  2.0    2.0     1.0  1.0  1.0   \n",
       "\n",
       "   very  plain  entirely  energy  predictable  the  laughs  most  powerful  \n",
       "+   2.0    1.0       1.0     1.0          1.0  3.0     1.0   2.0       2.0  \n",
       "-   2.0    2.0       2.0     2.0          2.0  1.0     2.0   1.0       1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counts_w_c = pd.DataFrame(counts_w_c, columns=V)\n",
    "df_counts_w_c.index = clss\n",
    "df_counts_w_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the denominator $\\sum_{w\\in V}(count(w,c)+\\alpha)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 34.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums_per_c = np.sum(counts_w_c, axis=1)  # the alpha was already considered in the initialisation of counts_w_c! \n",
    "sums_per_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\log P(w_{i}|c)$ per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.36729583, -3.36729583, -3.36729583, -3.36729583, -2.67414865,\n",
       "        -3.36729583, -3.36729583, -3.36729583, -2.67414865, -2.67414865,\n",
       "        -2.67414865, -2.67414865, -3.36729583, -3.36729583, -3.36729583,\n",
       "        -3.36729583, -2.26868354, -3.36729583, -2.67414865, -2.67414865],\n",
       "       [-2.42774824, -2.83321334, -2.83321334, -2.83321334, -3.52636052,\n",
       "        -2.83321334, -2.83321334, -2.83321334, -3.52636052, -3.52636052,\n",
       "        -3.52636052, -2.83321334, -2.83321334, -2.83321334, -2.83321334,\n",
       "        -2.83321334, -3.52636052, -2.83321334, -3.52636052, -3.52636052]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_w_c = np.log(counts_w_c / sums_per_c[:,None])\n",
    "log_likelihood_w_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "1. Convert document into word vector\n",
    "2. Calculate posterior probabilities $\\log P(c) + \\sum_{t_i} \\log P(w_i|c)$ for all classes $c$\n",
    "3. Select the maximum probability and map to the correspoding class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(doc):\n",
    "    # convert incoming document to word count vector\n",
    "    x = np.array(doc_to_word_counts(doc))\n",
    "    # calculate the posterior probabilities per class\n",
    "    # elements of x are 0 if the corresponding vocabulary word does not appear in doc and 1 if it does\n",
    "    # -> log_likelihood_w_c * x essentially selects the relevant log likelihoods\n",
    "    class_probs = log_priors + np.sum(log_likelihood_w_c * x, axis=1)\n",
    "    # select the class with the highest probability\n",
    "    class_index = np.argmax(class_probs)\n",
    "    # map class index back to label\n",
    "    return clss[class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('predictable with no fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
