{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with Naive Bayes\n",
    "Based on [Jurafsky: Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "\n",
    "Multinomial Naive Bayes: The class priors are assumed to follow a multinomial distribution - common for text classification.\n",
    "\n",
    "Assumes Bag-of-Words: Documents are represented by the unordered collection of their word counts. Text sequence information is ignored.\n",
    "## The Model\n",
    "* Vocabulary $V$: set of words $\\{w_j\\}$\n",
    "* Feature based on vocabulary word $w_j$ in document $d$: $x_{n,j}=f(w_j;D_n)$  \n",
    "* $K$ Classes $c_1,...,c_K$  \n",
    "* $N$ Documents\n",
    "The Naive Bayes classifier returns the class $\\hat{c}$ which has the maximum posterior probability given the document $D$.\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\hat{c}=\\argmax_{c \\in C} P(c|D)\n",
    "$$\n",
    "\n",
    "Bayes' Theorem says that the conditional probability $P(c|D)$ to observe class $c$ given document $D$ can be decomposed into the product of the likelihood $P(D|c)$ and the prior $P(c)$ (the probability for class $c$ without any observation)\n",
    "$$\\hat{c}=\\argmax_{c \\in C} P(c|D)=\\argmax_{c \\in C} \\frac{P(D|c)P(c)}{P(D)}$$\n",
    "\n",
    "$P(D)$ can be dropped, because it is the same for all classes.\n",
    "\n",
    "$$\\hat{c}=\\argmax_{c \\in C} P(c|D)=\\argmax_{c \\in C} \\overbrace{P(D|c)}^{\\text{likelihood}}\\overbrace{P(c)}^{\\text{prior}}$$\n",
    "\n",
    "Represent document $D$ as a set of features $f_1, f_2,...,f_J$, which encode the presence of the respective vocabulary words $w_1, w_2,...,w_J$  \n",
    "\n",
    "$$\\hat{c}=\\argmax_{c \\in C} P(f_1, f_2,...,f_J|c)P(c)$$\n",
    "\n",
    "Naive assumption: the probabilities $P(f_j|c)$ are independent given the class $c$ and therefore\n",
    "\n",
    "$$P(f_1, f_2,...,f_J|c) = P(f_j|c)\\cdot P(f_2|c)\\cdot ... \\cdot P(f_J|c)$$\n",
    "\n",
    "$$c_{NB}=\\argmax_{c \\in C} P(c) \\prod_{f} P(f|c)$$\n",
    "\n",
    "in log space to avoid underflow and increase speed:\n",
    "\n",
    "$$c_{NB}=\\argmax_{c \\in C} \\log P(c) + \\sum_{f} \\log P(f|c)$$\n",
    "\n",
    "Applied to word tokens of document $D$ $\\{t_i\\}$, where $w_i$ is the corresponding word in the vocabulary. Tokens $t_i$ without corresponding word in the vocabulary are simply ignored:\n",
    "$$c_{NB}=\\argmax_{c \\in C} \\log P(c) + \\sum_{t_i} \\log P(w_i|c)$$\n",
    "\n",
    "## Training\n",
    "\n",
    "$$\\hat{P}(c)=\\frac{N_c}{N}$$\n",
    "$N_c$ the number of documents of class $c$ divided by the total number of documents/samples.\n",
    "\n",
    "$P(w_{j}|c)$ is computed as the fraction of times the vocabulary word $w_j$ appears among all tokens in all documents of class $c$\n",
    "\n",
    "$$P(w_{i}|c)=\\frac{count(w_j,c)}{\\sum_{w_{j'}\\in V}count(w_{j'},c)}$$\n",
    "\n",
    "Apply Smoothing $\\alpha$ to avoid zero probabilites for vocabulary words that do not appear in all classes.\n",
    "\n",
    "$$P(w_{i}|c)=\\frac{count(w_i,c)+\\alpha}{\\sum_{w\\in V}(count(w,c)+\\alpha)}=\\frac{count(w_i,c)+\\alpha}{\\left(\\sum_{w\\in V}(count(w,c)\\right)+\\alpha\\cdot|V|}$$\n",
    "\n",
    "Laplace Smoothing: $\\alpha = 1$  \n",
    "Lidstone Smoothing: $0 < \\alpha < 1$ \n",
    "\n",
    "Words missing in the vocabulary are simply ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Sentiment Classification task (0: Negative, 1: Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['just plain boring',\n",
    "         'entirely predictable and lacks energy',\n",
    "         'no surprises and very few laughs',\n",
    "         'very powerful',\n",
    "         'the most fun file of the summer'\n",
    "        ]\n",
    "\n",
    "classes = ['-',\n",
    "     '-',\n",
    "     '-',\n",
    "     '+',\n",
    "     '+'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs)\n",
    "clss = list(set(classes))\n",
    "K = len(clss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Illustrative Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organise the Target Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_cls = {label:i for i,label in enumerate(clss)}\n",
    "\n",
    "T = np.array([map_cls[label] for label in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', '-']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Log Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.6]\n",
      "[-0.916290731874155, -0.5108256237659907]\n"
     ]
    }
   ],
   "source": [
    "log_priors = [0.0]*K\n",
    "for t in T:\n",
    "    log_priors[t] += 1\n",
    "print(priors)\n",
    "log_priors = [np.log(count/N) for count in log_priors]\n",
    "print(log_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set([token for sent in sents for token in sent.split()])\n",
    "n_V = len(V)\n",
    "\n",
    "map_V = {word:pos for pos, word in enumerate(V)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'of': 1,\n",
       " 'plain': 2,\n",
       " 'laughs': 3,\n",
       " 'no': 4,\n",
       " 'summer': 5,\n",
       " 'energy': 6,\n",
       " 'just': 7,\n",
       " 'boring': 8,\n",
       " 'entirely': 9,\n",
       " 'very': 10,\n",
       " 'few': 11,\n",
       " 'fun': 12,\n",
       " 'surprises': 13,\n",
       " 'file': 14,\n",
       " 'the': 15,\n",
       " 'lacks': 16,\n",
       " 'most': 17,\n",
       " 'predictable': 18,\n",
       " 'powerful': 19}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training documents into word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_word_counts(doc):\n",
    "    counts = [0]*n_V\n",
    "    for token in doc.split():\n",
    "        try:\n",
    "            counts[map_V[token]] += 1\n",
    "        except KeyError: # simply ignore words not in the vocabulary\n",
    "            pass\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([doc_to_word_counts(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ now contains for each document (row) the counts per vocabulary word (column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>plain</th>\n",
       "      <th>laughs</th>\n",
       "      <th>no</th>\n",
       "      <th>summer</th>\n",
       "      <th>energy</th>\n",
       "      <th>just</th>\n",
       "      <th>boring</th>\n",
       "      <th>entirely</th>\n",
       "      <th>very</th>\n",
       "      <th>few</th>\n",
       "      <th>fun</th>\n",
       "      <th>surprises</th>\n",
       "      <th>file</th>\n",
       "      <th>the</th>\n",
       "      <th>lacks</th>\n",
       "      <th>most</th>\n",
       "      <th>predictable</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just plain boring</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entirely predictable and lacks energy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no surprises and very few laughs</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very powerful</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the most fun file of the summer</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  of  plain  laughs  no  summer  \\\n",
       "just plain boring                        0   0      1       0   0       0   \n",
       "entirely predictable and lacks energy    1   0      0       0   0       0   \n",
       "no surprises and very few laughs         1   0      0       1   1       0   \n",
       "very powerful                            0   0      0       0   0       0   \n",
       "the most fun file of the summer          0   1      0       0   0       1   \n",
       "\n",
       "                                       energy  just  boring  entirely  very  \\\n",
       "just plain boring                           0     1       1         0     0   \n",
       "entirely predictable and lacks energy       1     0       0         1     0   \n",
       "no surprises and very few laughs            0     0       0         0     1   \n",
       "very powerful                               0     0       0         0     1   \n",
       "the most fun file of the summer             0     0       0         0     0   \n",
       "\n",
       "                                       few  fun  surprises  file  the  lacks  \\\n",
       "just plain boring                        0    0          0     0    0      0   \n",
       "entirely predictable and lacks energy    0    0          0     0    0      1   \n",
       "no surprises and very few laughs         1    0          1     0    0      0   \n",
       "very powerful                            0    0          0     0    0      0   \n",
       "the most fun file of the summer          0    1          0     1    2      0   \n",
       "\n",
       "                                       most  predictable  powerful  \n",
       "just plain boring                         0            0         0  \n",
       "entirely predictable and lacks energy     0            1         0  \n",
       "no surprises and very few laughs          0            0         0  \n",
       "very powerful                             0            0         1  \n",
       "the most fun file of the summer           1            0         0  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X = pd.DataFrame(X, columns=V)\n",
    "df_X.index = docs\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Calculate the Likelihoods for the vocabulary words given class $c$\n",
    "$$P(w_{i}|c)=\\frac{count(w_i,c)+\\alpha}{\\sum_{w\\in V}(count(w,c)+\\alpha)}$$\n",
    "\n",
    "First calculate the  $count(w_i,c)+\\alpha$ for each class $c$ and vocabulary word $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_w_c = np.ones((K,n_V)) * alpha  # initialise with the +alpha counts for the Smoothing\n",
    "for label,i in map_cls.items():  # loop over all labels and their positions in the cls array\n",
    "    for doc in X[T == i]:  # select all docs of class c\n",
    "        counts_w_c[i] += doc  # and add up the word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>plain</th>\n",
       "      <th>laughs</th>\n",
       "      <th>no</th>\n",
       "      <th>summer</th>\n",
       "      <th>energy</th>\n",
       "      <th>just</th>\n",
       "      <th>boring</th>\n",
       "      <th>entirely</th>\n",
       "      <th>very</th>\n",
       "      <th>few</th>\n",
       "      <th>fun</th>\n",
       "      <th>surprises</th>\n",
       "      <th>file</th>\n",
       "      <th>the</th>\n",
       "      <th>lacks</th>\n",
       "      <th>most</th>\n",
       "      <th>predictable</th>\n",
       "      <th>powerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and   of  plain  laughs   no  summer  energy  just  boring  entirely  very  \\\n",
       "+  1.0  2.0    1.0     1.0  1.0     2.0     1.0   1.0     1.0       1.0   2.0   \n",
       "-  3.0  1.0    2.0     2.0  2.0     1.0     2.0   2.0     2.0       2.0   2.0   \n",
       "\n",
       "   few  fun  surprises  file  the  lacks  most  predictable  powerful  \n",
       "+  1.0  2.0        1.0   2.0  3.0    1.0   2.0          1.0       2.0  \n",
       "-  2.0  1.0        2.0   1.0  1.0    2.0   1.0          2.0       1.0  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counts_w_c = pd.DataFrame(counts_w_c, columns=V)\n",
    "df_counts_w_c.index = clss\n",
    "df_counts_w_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the denominator $\\sum_{w\\in V}(count(w,c)+\\alpha)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 34.])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums_per_c = np.sum(counts_w_c, axis=1)  # the alpha was already considered in the initialisation of counts_w_c! \n",
    "sums_per_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\log P(w_{i}|c)$ per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.36729583, -2.67414865, -3.36729583, -3.36729583, -3.36729583,\n",
       "        -2.67414865, -3.36729583, -3.36729583, -3.36729583, -3.36729583,\n",
       "        -2.67414865, -3.36729583, -2.67414865, -3.36729583, -2.67414865,\n",
       "        -2.26868354, -3.36729583, -2.67414865, -3.36729583, -2.67414865],\n",
       "       [-2.42774824, -3.52636052, -2.83321334, -2.83321334, -2.83321334,\n",
       "        -3.52636052, -2.83321334, -2.83321334, -2.83321334, -2.83321334,\n",
       "        -2.83321334, -2.83321334, -3.52636052, -2.83321334, -3.52636052,\n",
       "        -3.52636052, -2.83321334, -3.52636052, -2.83321334, -3.52636052]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_w_c = np.log(counts_w_c / sums_per_c[:,None])\n",
    "log_likelihood_w_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "1. Convert document into word vector\n",
    "2. Calculate posterior probabilities $\\log P(c) + \\sum_{t_i} \\log P(w_i|c)$ for all classes $c$\n",
    "3. Select the maximum probability and map to the correspoding class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(doc):\n",
    "    # convert incoming document to word count vector\n",
    "    x = np.array(doc_to_word_counts(doc))\n",
    "    # calculate the posterior probabilities per class\n",
    "    # elements of x are 0 if the corresponding vocabulary word does not appear in doc and 1 if it does\n",
    "    # -> log_likelihood_w_c * x essentially selects the relevant log likelihoods\n",
    "    class_probs = log_priors + np.sum(log_likelihood_w_c * x, axis=1)\n",
    "    # select the class with the highest probability\n",
    "    class_index = np.argmax(class_probs)\n",
    "    # map class index back to label\n",
    "    return clss[class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('predictable with no fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
